{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import SimpleITK as sitk\n",
    "# import jax.numpy as jnp\n",
    "# import itertools\n",
    "\n",
    "\n",
    "# def join_ct_suv(ct: sitk.Image, suv: sitk.Image,ct1: sitk.Image, suv1: sitk.Image) -> sitk.Image:\n",
    "#     '\n",
    "#     Resample a CT image to the same size as a SUV image\n",
    "#     '\n",
    "   \n",
    "#     ct_arr=sitk.GetArrayFromImage(ct)\n",
    "#     suv_arr=sitk.GetArrayFromImage(suv)\n",
    "\n",
    "#     ct_arr_1=sitk.GetArrayFromImage(ct1)\n",
    "#     suv_arr_1=sitk.GetArrayFromImage(suv1)\n",
    "    \n",
    "#     res=jnp.stack([jnp.array(suv_arr),jnp.array(ct_arr),jnp.array(ct_arr_1),jnp.array(suv_arr_1)],axis=-1)\n",
    "#     return res\n",
    "\n",
    "# def load_landmark_data(folder_path:str):\n",
    "#     '\n",
    "#     given path to folder with landmarks files and images after general registaration we load the data\n",
    "#     we want to first load the suv and ct images resample them to the same size and then load the landmarks\n",
    "#     we need to load separately study 0 and 1 \n",
    "#     the output should be in form of a dictionary with keys 'study_0','study_1','From`,`To`' where `From` and `To` are the landmarks\n",
    "#     all the data should be in form of jnp.arrays\n",
    "#     '\n",
    "#     ct_0=sitk.ReadImage(folder_path+'/study_0_ct_soft.nii.gz')\n",
    "#     suv_0=sitk.ReadImage(folder_path+'/study_0_SUVS.nii.gz')\n",
    "#     # Resample ct_0 to match ct_1\n",
    "            \n",
    "#     ct_1=sitk.ReadImage(folder_path+'/study_1_ct_soft.nii.gz')\n",
    "#     suv_1=sitk.ReadImage(folder_path+'/study_1_SUVS.nii.gz')    \n",
    "#     arr_0 = join_ct_suv(ct_0, suv_0,ct_1, suv_1)\n",
    "\n",
    "#     return {'study':arr_0, 'From':jnp.load(folder_path+'/From.npy'),'To':jnp.load(folder_path+'/To.npy')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import numpy as np\n",
    "# import numpy as np\n",
    "# folder_path='/root/data/prepared_registered'\n",
    "# # folder_path='/root/data/prepared_registered/pat_2/general_transform'\n",
    "# # load_landmark_data(folder_path)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def reshape_image(arr, img_size):\n",
    "#     # Get the current shape of the input array\n",
    "#     img_size=(img_size[1],img_size[2],img_size[3],img_size[4])\n",
    "#     current_shape = arr.shape\n",
    "    \n",
    "#     # Check if the current shape is already equal to the desired shape\n",
    "#     if current_shape == img_size:\n",
    "#         print(\"The input array already has the desired shape.\")\n",
    "#         return arr\n",
    "    \n",
    "#     # Check if the current shape is larger than the desired shape in any dimension\n",
    "#     if any(cs > ds for cs, ds in zip(current_shape, img_size)):\n",
    "#         # Crop the input array from the end of the dimension where it occurs\n",
    "#         arr = arr[:img_size[0], :img_size[1], :img_size[2], :img_size[3]]\n",
    "#         print(\"The input array has been cropped to the desired shape.\")\n",
    "    \n",
    "#     # Check if the current shape is smaller than the desired shape in any dimension\n",
    "#     if any(cs < ds for cs, ds in zip(current_shape, img_size)):\n",
    "#         # Pad the input array with zeros at the end of the dimension where it occurs\n",
    "\n",
    "#         arr = np.pad(arr, ((0, np.max(img_size[0] - current_shape[0],0)),\n",
    "#                                   (0, np.max(img_size[1] - current_shape[1],0)),\n",
    "#                                   (0, np.max(img_size[2] - current_shape[2],0)),\n",
    "#                                   (0, 0)), mode='constant')\n",
    "#         print(\"The input array has been padded to the desired shape.\")\n",
    "    \n",
    "#     # If none of the above conditions are met, return the input array as is\n",
    "#     return arr\n",
    "\n",
    "# batch_size=2\n",
    "# img_size = (batch_size,488, 200, 200,2)\n",
    "\n",
    "# def stack_with_pad(arr_0,arr_1):\n",
    "#     if arr_0.shape[0] > arr_1.shape[0]:\n",
    "#         pad_length = arr_0.shape[0] - arr_1.shape[0]\n",
    "#         padding = jnp.full((pad_length, arr_1.shape[1]), -1)\n",
    "#         arr_1 = jnp.concatenate((arr_1, padding), axis=0)\n",
    "#     elif arr_1.shape[0] > arr_0.shape[0]:\n",
    "#         pad_length = arr_1.shape[0] - arr_0.shape[0]\n",
    "#         padding = jnp.full((pad_length, arr_0.shape[1]), -1)\n",
    "#         arr_0 = jnp.concatenate((arr_0, padding), axis=0)\n",
    "    \n",
    "#     return jnp.stack([arr_0, arr_1])\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "# def get_batched(folder_tuple):\n",
    "#     folder_0=load_landmark_data(f\"{folder_tuple[0]}/general_transform\")\n",
    "#     folder_1=load_landmark_data(f\"{folder_tuple[1]}/general_transform\")\n",
    "#     arr=jnp.stack([reshape_image(folder_0['study'],img_size),reshape_image(folder_1['study'],img_size)])\n",
    "#     From=stack_with_pad(folder_0['From'],folder_1['From'])\n",
    "#     To=stack_with_pad(folder_0['To'],folder_1['To'])\n",
    "#     return {'study':arr, 'From':From,'To':To}\n",
    "\n",
    "\n",
    "# # folder_tuples = list(itertools.zip_longest(*[iter(folder_names)] * 2))\n",
    "# # tt=list(map(get_batched,folder_tuples))\n",
    "\n",
    "# # tt=list(map(lambda el: reshape_image(load_landmark_data(f\"{el}/general_transform\")['study'],img_size) ,folder_names))\n",
    "\n",
    "# # create a function that given input array'arr' will change it shape to shape given as 'img_size' if the given image is bigger than 'img_size' in any dimension image should be cropped from the end of dimension where it happend in case when image is bigger than 'img_size' image should be padded with zeros at the end of dimension where it happens ; check weather the resulting image has shape required\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['pat_id', 'lesion_num', 'study_0_or_1', 'Deauville', 'lab_path',\n",
       "       'mod_name', 'vol_in_mm3', 'original_firstorder_10Percentile_pet',\n",
       "       'original_firstorder_90Percentile_pet',\n",
       "       'original_firstorder_Energy_pet',\n",
       "       ...\n",
       "       'wavelet-LLL_glszm_LargeAreaLowGrayLevelEmphasis_ct',\n",
       "       'wavelet-LLL_glszm_LowGrayLevelZoneEmphasis_ct',\n",
       "       'wavelet-LLL_glszm_SizeZoneNonUniformity_ct',\n",
       "       'wavelet-LLL_glszm_SizeZoneNonUniformityNormalized_ct',\n",
       "       'wavelet-LLL_glszm_SmallAreaEmphasis_ct',\n",
       "       'wavelet-LLL_glszm_SmallAreaHighGrayLevelEmphasis_ct',\n",
       "       'wavelet-LLL_glszm_SmallAreaLowGrayLevelEmphasis_ct',\n",
       "       'wavelet-LLL_glszm_ZoneEntropy_ct',\n",
       "       'wavelet-LLL_glszm_ZonePercentage_ct',\n",
       "       'wavelet-LLL_glszm_ZoneVariance_ct'],\n",
       "      dtype='object', length=1879)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "radiomics_full_data_path=\"/workspaces/pilot_lymphoma/data/extracted_features_pet_trimmedB.csv\"\n",
    "radiomics_full_data=pd.read_csv(radiomics_full_data_path)\n",
    "radiomics_full_data = radiomics_full_data.loc[:, ~radiomics_full_data.columns.str.contains('Unnamed', case=False)]\n",
    "radiomics_full_data = radiomics_full_data[radiomics_full_data['lesion_num'] == 1000]\n",
    "radiomics_full_data[\"pat_id\"]=radiomics_full_data[\"pat_id\"].astype(int)\n",
    "radiomics_full_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([<bound method IndexOpsMixin.to_numpy of 0     PR\n",
       "       1     CR\n",
       "       2     PR\n",
       "       3     CR\n",
       "       4     PD\n",
       "       5     CR\n",
       "       6     CR\n",
       "       7     SD\n",
       "       8     CR\n",
       "       9     CR\n",
       "       10    PD\n",
       "       11    PD\n",
       "       12    PR\n",
       "       13    PD\n",
       "       14    PR\n",
       "       15    SD\n",
       "       16    CR\n",
       "       17    CR\n",
       "       18    CR\n",
       "       19    CR\n",
       "       20    CR\n",
       "       21    CR\n",
       "       22    SD\n",
       "       23    PD\n",
       "       24    CR\n",
       "       25    PD\n",
       "       26    CR\n",
       "       27    CR\n",
       "       28    PD\n",
       "       29    CR\n",
       "       30    CR\n",
       "       31    PR\n",
       "       Name: outcome, dtype: object>                   ], dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_data_table_path=\"/workspaces/pilot_lymphoma/data/full_table_data_for_delta.csv\"\n",
    "full_data_table= pd.read_csv(full_data_table_path)\n",
    "full_data_table[\"pat_id\"]=full_data_table[\"Unnamed: 0\"].astype(int)\n",
    "full_data_table[\"outcome\"]=full_data_table[\"Unnamed: 12\"]\n",
    "np.unique(full_data_table[\"outcome\"].to_numpy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ggggggggggggg 0  1\n",
      "ggggggggggggg 1  8\n",
      "ggggggggggggg 1  10\n",
      "ggggggggggggg 1  14\n",
      "ggggggggggggg 1  15\n",
      "ggggggggggggg 1  16\n",
      "ggggggggggggg 0  17\n",
      "ggggggggggggg 1  18\n",
      "ggggggggggggg 1  21\n",
      "ggggggggggggg 0  26\n",
      "ggggggggggggg 0  27\n",
      "ggggggggggggg 0  32\n",
      "ggggggggggggg 0  33\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['vol_in_mm3', 'original_firstorder_10Percentile_pet',\n",
       "       'original_firstorder_90Percentile_pet',\n",
       "       'original_firstorder_Energy_pet', 'original_firstorder_Entropy_pet',\n",
       "       'original_firstorder_InterquartileRange_pet',\n",
       "       'original_firstorder_Kurtosis_pet', 'original_firstorder_Maximum_pet',\n",
       "       'original_firstorder_MeanAbsoluteDeviation_pet',\n",
       "       'original_firstorder_Mean_pet',\n",
       "       ...\n",
       "       'wavelet-LLL_glszm_LowGrayLevelZoneEmphasis_ct',\n",
       "       'wavelet-LLL_glszm_SizeZoneNonUniformity_ct',\n",
       "       'wavelet-LLL_glszm_SizeZoneNonUniformityNormalized_ct',\n",
       "       'wavelet-LLL_glszm_SmallAreaEmphasis_ct',\n",
       "       'wavelet-LLL_glszm_SmallAreaHighGrayLevelEmphasis_ct',\n",
       "       'wavelet-LLL_glszm_SmallAreaLowGrayLevelEmphasis_ct',\n",
       "       'wavelet-LLL_glszm_ZoneEntropy_ct',\n",
       "       'wavelet-LLL_glszm_ZonePercentage_ct',\n",
       "       'wavelet-LLL_glszm_ZoneVariance_ct', 'outcome'],\n",
       "      dtype='object', length=1874)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def subtract_dicts(dict1, dict2,dict_sums):\n",
    "    # Create a new dictionary with the absolute difference of each entry\n",
    "    result = {key: abs(dict1[key] - dict2[key])/dict_sums[key] for key in dict1}\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "def get_delta_radiomics(full_data_table_row, radiomics_full_data):\n",
    "\n",
    "    \"\"\"a function 'get_delta_radiomics' that would have two arguments 'full_data_table_row' and 'radiomics_full_data'.  'full_data_table_row' is a row from main table and contains columns like '[pat_id,outcome]'   'radiomics_full_data' contains multiple column including'[pat_id,study_0_or_1]'  Function should perform all steps:\n",
    "    1) find 2 rows from 'radiomics_full_data' where  value of column 'pat_id' would be the same as value of column 'pat_id' in 'full_data_table_row' \n",
    "    2) From those 2 rows you found drop columns with names: ```['pat_id', 'lesion_num', 'study_0_or_1', 'Deauville', 'lab_path', 'mod_name']```\n",
    "    3) Save the sum of both rows so each column should have sum of 2 rows\n",
    "    4) calculate the absolute value of the diffrence between two rows and divide it by the saved sum save information as dictionary called 'res'\n",
    "    5) add outcome variable to 'res' that you will find in column 'outcome' in 'full_data_table_row' encode the  'outcome' as integer as in the dictionary {'CR':0, 'PD':1, 'PR':2, 'SD':2, }\n",
    "    6) return calculated dictionary res\"\"\"\n",
    "    full_data_table_row=full_data_table_row[1]\n",
    "    # print(f\"pppp {full_data_table_row['pat_id']}\")\n",
    "    # Step 1\n",
    "    rows = radiomics_full_data[radiomics_full_data['pat_id'] == full_data_table_row['pat_id']]\n",
    "    if(len(rows))<2:\n",
    "        print(f\"ggggggggggggg {len(rows)}  {full_data_table_row['pat_id']}\")\n",
    "        return \" \"\n",
    "    # Step 2\n",
    "    rows = rows.drop(columns=['pat_id', 'lesion_num', 'study_0_or_1', 'Deauville', 'lab_path', 'mod_name'])\n",
    "    # Step 3\n",
    "    row_sum = rows.sum().to_dict()\n",
    "   \n",
    "    # print(len(rows))\n",
    "    # Step 4\n",
    "    res = subtract_dicts(rows.iloc[0].to_dict(),rows.iloc[1].to_dict(),row_sum )\n",
    "    # print(res)\n",
    "    # Step 5\n",
    "    outcome_dict = {'CR':0, 'PD':1, 'PR':2, 'SD':2}\n",
    "    res['outcome'] = outcome_dict[full_data_table_row['outcome']]\n",
    "\n",
    "    # Step 6\n",
    "    return res\n",
    "\n",
    "rows = list(full_data_table.iterrows())\n",
    "\n",
    "\n",
    "delta_r=list(map( lambda el: get_delta_radiomics(el, radiomics_full_data),rows))\n",
    "delta_r= [el for el in delta_r if el != \" \"]\n",
    "delta_r=pd.DataFrame(delta_r)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     0\n",
       "1     2\n",
       "2     0\n",
       "3     1\n",
       "4     0\n",
       "5     0\n",
       "6     0\n",
       "7     1\n",
       "8     1\n",
       "9     2\n",
       "10    0\n",
       "11    0\n",
       "12    0\n",
       "13    2\n",
       "14    1\n",
       "15    0\n",
       "16    0\n",
       "17    1\n",
       "18    0\n",
       "Name: outcome, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# np.unique(delta_r['outcome'])\n",
    "\n",
    "delta_r['outcome']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check what is the sum of euclidean distances for diffrent registrations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/root/data/prepared_registered/pat_12',\n",
       " '/root/data/prepared_registered/pat_16',\n",
       " '/root/data/prepared_registered/pat_31',\n",
       " '/root/data/prepared_registered/pat_21',\n",
       " '/root/data/prepared_registered/pat_26',\n",
       " '/root/data/prepared_registered/pat_2',\n",
       " '/root/data/prepared_registered/pat_19',\n",
       " '/root/data/prepared_registered/pat_13',\n",
       " '/root/data/prepared_registered/pat_10',\n",
       " '/root/data/prepared_registered/pat_28',\n",
       " '/root/data/prepared_registered/pat_8',\n",
       " '/root/data/prepared_registered/pat_24',\n",
       " '/root/data/prepared_registered/pat_15',\n",
       " '/root/data/prepared_registered/pat_4',\n",
       " '/root/data/prepared_registered/pat_29',\n",
       " '/root/data/prepared_registered/pat_14',\n",
       " '/root/data/prepared_registered/pat_20',\n",
       " '/root/data/prepared_registered/pat_22',\n",
       " '/root/data/prepared_registered/pat_5',\n",
       " '/root/data/prepared_registered/pat_18',\n",
       " '/root/data/prepared_registered/pat_11',\n",
       " '/root/data/prepared_registered/pat_9',\n",
       " '/root/data/prepared_registered/pat_27',\n",
       " '/root/data/prepared_registered/pat_3',\n",
       " '/root/data/prepared_registered/pat_23',\n",
       " '/root/data/prepared_registered/pat_7',\n",
       " '/root/data/prepared_registered/pat_25',\n",
       " '/root/data/prepared_registered/pat_6']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folder_names = [os.path.join(folder_path, name) for name in os.listdir(folder_path) if os.path.isdir(os.path.join(folder_path, name))]\n",
    "folder_names= list(filter(lambda el: \"pat\" in el, folder_names))\n",
    "folder_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27.419910917593374"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### checking from linear folder the distance\n",
    "def get_dist_0(fold_name):\n",
    "  fold=f\"{fold_name}/lin_transf\"\n",
    "  fromm=np.load(f\"{fold}/From.npy\")\n",
    "  too=np.load(f\"{fold}/To.npy\")\n",
    "\n",
    "  res=(fromm-too)\n",
    "  res=res*(fromm>0)\n",
    "  res=np.sqrt(np.sum(res**2,axis=-1))\n",
    "  return np.sum(res.flatten())\n",
    "\n",
    "np.mean(list(map(get_dist_0,folder_names)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00,  4.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selected_features ['wavelet-HHH_glcm_Imc2_pet', 'log-sigma-4-0-mm-3D_firstorder_90Percentile_pet', 'wavelet-LLH_glszm_LargeAreaHighGrayLevelEmphasis_ct']\n",
      "[iter 0] loss=0.7205 val_loss=0.0000 scale=8.0000 norm=17.4545\n",
      "[iter 0] loss=0.5945 val_loss=0.0000 scale=4.0000 norm=7.6364\n",
      "[iter 0] loss=0.5945 val_loss=0.0000 scale=512.0000 norm=977.4545\n",
      "[iter 0] loss=0.6575 val_loss=0.0000 scale=512.0000 norm=1047.2727\n",
      "[iter 0] loss=0.6575 val_loss=0.0000 scale=512.0000 norm=1047.2727\n",
      "AAA Accuracy: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00,  4.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selected_features ['wavelet-HHH_glcm_Imc2_pet', 'wavelet-LLL_glszm_LowGrayLevelZoneEmphasis_pet', 'wavelet-HHH_firstorder_Mean_pet']\n",
      "[iter 0] loss=0.5860 val_loss=0.0000 scale=8.0000 norm=16.1157\n",
      "[iter 0] loss=0.4941 val_loss=0.0000 scale=512.0000 norm=920.3306\n",
      "AAA Accuracy: 0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00,  4.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selected_features ['wavelet-HHH_glcm_Imc1_pet', 'log-sigma-3-0-mm-3D_firstorder_Kurtosis_pet', 'wavelet-HHL_glszm_GrayLevelNonUniformityNormalized_ct']\n",
      "[iter 0] loss=0.5860 val_loss=0.0000 scale=2.0000 norm=4.0289\n",
      "[iter 0] loss=0.5860 val_loss=0.0000 scale=4.0000 norm=8.0579\n",
      "[iter 0] loss=0.5860 val_loss=0.0000 scale=2.0000 norm=4.0289\n",
      "[iter 0] loss=0.5860 val_loss=0.0000 scale=32.0000 norm=64.4628\n",
      "[iter 0] loss=0.6780 val_loss=0.0000 scale=2.0000 norm=4.4628\n",
      "[iter 0] loss=0.5860 val_loss=0.0000 scale=2.0000 norm=4.0289\n",
      "[iter 0] loss=0.4941 val_loss=0.0000 scale=512.0000 norm=920.3306\n",
      "[iter 0] loss=0.5860 val_loss=0.0000 scale=2.0000 norm=4.0289\n",
      "[iter 0] loss=0.5860 val_loss=0.0000 scale=512.0000 norm=1031.4050\n",
      "[iter 0] loss=0.5860 val_loss=0.0000 scale=16.0000 norm=32.2314\n",
      "[iter 0] loss=0.5860 val_loss=0.0000 scale=512.0000 norm=1031.4050\n",
      "[iter 0] loss=0.5860 val_loss=0.0000 scale=2.0000 norm=4.0289\n",
      "[iter 0] loss=0.5860 val_loss=0.0000 scale=2.0000 norm=4.0289\n",
      "AAA Accuracy: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00,  4.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selected_features ['wavelet-HHH_glcm_Imc1_pet', 'wavelet-HHL_glszm_GrayLevelVariance_ct', 'log-sigma-5-0-mm-3D_glszm_LargeAreaLowGrayLevelEmphasis_ct']\n",
      "[iter 0] loss=0.6780 val_loss=0.0000 scale=2.0000 norm=4.4628\n",
      "[iter 0] loss=0.5860 val_loss=0.0000 scale=8.0000 norm=16.1157\n",
      "[iter 0] loss=0.5860 val_loss=0.0000 scale=4.0000 norm=8.0579\n",
      "[iter 0] loss=0.5860 val_loss=0.0000 scale=8.0000 norm=16.1157\n",
      "[iter 0] loss=0.5860 val_loss=0.0000 scale=512.0000 norm=1031.4050\n",
      "[iter 0] loss=0.4941 val_loss=0.0000 scale=8.0000 norm=14.3802\n",
      "AAA Accuracy: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00,  3.91it/s]"
     ]
    }
   ],
   "source": [
    "import SimpleITK as sitk\n",
    "import six\n",
    "from radiomics import featureextractor, getTestCase\n",
    "import multiprocessing\n",
    "import optuna\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from ngboost import NGBRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from ngboost.distns import Exponential, Normal\n",
    "from ngboost import NGBClassifier\n",
    "from ngboost.distns import k_categorical, Bernoulli\n",
    "from mrmr import mrmr_classif\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import shap\n",
    "import sklearn\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "def subtract_dicts(dict1, dict2,dict_sums):\n",
    "    # Create a new dictionary with the absolute difference of each entry\n",
    "    result = {key: abs(dict1[key] - dict2[key])/dict_sums[key] for key in dict1}\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "def get_delta_radiomics(full_data_table_row, radiomics_full_data):\n",
    "\n",
    "    \"\"\"a function 'get_delta_radiomics' that would have two arguments 'full_data_table_row' and 'radiomics_full_data'.  'full_data_table_row' is a row from main table and contains columns like '[pat_id,outcome]'   'radiomics_full_data' contains multiple column including'[pat_id,study_0_or_1]'  Function should perform all steps:\n",
    "    1) find 2 rows from 'radiomics_full_data' where  value of column 'pat_id' would be the same as value of column 'pat_id' in 'full_data_table_row' \n",
    "    2) From those 2 rows you found drop columns with names: ```['pat_id', 'lesion_num', 'study_0_or_1', 'Deauville', 'lab_path', 'mod_name']```\n",
    "    3) Save the sum of both rows so each column should have sum of 2 rows\n",
    "    4) calculate the absolute value of the diffrence between two rows and divide it by the saved sum save information as dictionary called 'res'\n",
    "    5) add outcome variable to 'res' that you will find in column 'outcome' in 'full_data_table_row' encode the  'outcome' as integer as in the dictionary {'CR':0, 'PD':1, 'PR':2, 'SD':2, }\n",
    "    6) return calculated dictionary res\"\"\"\n",
    "    full_data_table_row=full_data_table_row[1]\n",
    "    # print(f\"pppp {full_data_table_row['pat_id']}\")\n",
    "    # Step 1\n",
    "    rows = radiomics_full_data[radiomics_full_data['pat_id'] == full_data_table_row['pat_id']]\n",
    "    if(len(rows))<2:\n",
    "        # print(f\"ggggggggggggg {len(rows)}  {full_data_table_row['pat_id']}\")\n",
    "        return \" \"\n",
    "    # Step 2\n",
    "    rows = rows.drop(columns=['pat_id', 'lesion_num', 'study_0_or_1', 'Deauville', 'lab_path', 'mod_name'])\n",
    "    # Step 3\n",
    "    row_sum = rows.sum().to_dict()\n",
    "   \n",
    "    # print(len(rows))\n",
    "    # Step 4\n",
    "    res = subtract_dicts(rows.iloc[0].to_dict(),rows.iloc[1].to_dict(),row_sum )\n",
    "    # print(res)\n",
    "    # Step 5\n",
    "    outcome_dict = {'CR':0, 'PD':1, 'PR':0, 'SD':0}\n",
    "    res['outcome'] = outcome_dict[full_data_table_row['outcome']]\n",
    "\n",
    "    # Step 6\n",
    "    return res\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# mrmr_selection,shap,ngboost\n",
    "\n",
    "\n",
    "def display_probs(curr_class, inferred_probs, Y_test,to_be_sorted=True):\n",
    "\n",
    "    probd_curr=inferred_probs[:,curr_class]\n",
    "    class_curr=(Y_test==curr_class)\n",
    "    if(to_be_sorted):\n",
    "        # Concatenate probd_curr and class_curr\n",
    "        combined = np.column_stack((probd_curr, class_curr))\n",
    "\n",
    "        # Sort by probd_curr\n",
    "        combined_sorted = combined[combined[:, 0].argsort()[::-1]]\n",
    "\n",
    "        # Divide back into probd_curr and class_curr\n",
    "        probd_curr = combined_sorted[:, 0]\n",
    "        class_curr = combined_sorted[:, 1]\n",
    "\n",
    "    # Set the colors for the columns\n",
    "    colors = ['red' if c == 0 else 'green' for c in class_curr]\n",
    "    # Plot the column plot\n",
    "    plt.bar(range(len(probd_curr)), probd_curr, color=colors)\n",
    "\n",
    "    # Add a vertical line at value 0.5\n",
    "    plt.axhline(y=0.5, color='blue', linestyle='--')\n",
    "\n",
    "\n",
    "\n",
    "    # Add labels and title\n",
    "    plt.xlabel('Index')\n",
    "    plt.ylabel('Probability')\n",
    "    plt.title(f'class {curr_class}')\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "def display_feature_importance(ngb,X_train):\n",
    "\n",
    "    shap.initjs()\n",
    "\n",
    "    ## SHAP plot for loc trees\n",
    "    explainer = shap.TreeExplainer(ngb, model_output=0) # use model_output = 1 for scale trees\n",
    "    shap_values = explainer.shap_values(X_train)\n",
    "    shap.summary_plot(shap_values, X_train, feature_names=X_train.columns.to_numpy())\n",
    "\n",
    "\n",
    "\n",
    "def get_tree_hyper_params(trial):\n",
    "    criterion= \"friedman_mse\"#trial.suggest_categorical(\"criterion\", [\"squared_error\", \"friedman_mse\", \"absolute_error\", \"poisson\"])\n",
    "    splitter=\"random\"#trial.suggest_categorical(\"splitter\", [\"best\",\"random\"])\n",
    "    max_features=None#trial.suggest_categorical(\"max_features\", [\"sqrt\",\"log2\",None])\n",
    "    # max_leaf_nodesint=trial.suggest_categorical(\"max_leaf_nodesint\", [])\n",
    "    max_depth=2#trial.suggest_int(\"max_depth\", 1,10)\n",
    "    min_samples_leaf=1#trial.suggest_int(\"min_samples_leaf\", 1,3)\n",
    "    min_impurity_decrease= 0.2307277162959608#trial.suggest_float(\"min_impurity_decrease\", 0.0,0.3)\n",
    "\n",
    "    return sklearn.tree.DecisionTreeRegressor(criterion=criterion,splitter=splitter,max_depth=max_depth,max_features=max_features,min_samples_leaf=min_samples_leaf\n",
    "                                              ,min_impurity_decrease=min_impurity_decrease)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def clasify( main_df_val,main_df_train,y_cols,chosen_y_col,num_classes,K,to_display,Base,n_estimators,learning_rate,minibatch_frac):\n",
    "\n",
    "    main_df_val[chosen_y_col]=main_df_val[chosen_y_col].to_numpy().astype(int)\n",
    "    main_df_train[chosen_y_col]=main_df_train[chosen_y_col].to_numpy().astype(int)\n",
    "    \n",
    "    # main_df_val = main_df_val[main_df_val[chosen_y_col] > -1]\n",
    "    # main_df_train = main_df_train[main_df_train[chosen_y_col] > -1]\n",
    "\n",
    "    # print(f\"mmmm val {len(main_df_val)} train {len(main_df_train)}\")\n",
    "\n",
    "    X_train = main_df_train.drop(columns=y_cols )\n",
    "    X_test = main_df_val.drop(columns=y_cols)\n",
    "\n",
    "    X_train = X_train.iloc[:, 1:]\n",
    "    X_test = X_train.iloc[:, 1:]\n",
    "\n",
    "    Y_train = main_df_train[chosen_y_col]\n",
    "    Y_test = main_df_val[chosen_y_col]\n",
    "\n",
    "\n",
    "    # select top K features using mRMR\n",
    "    selected_features = mrmr_classif(X=X_train, y=Y_train, K=3,n_jobs=1)\n",
    "    # selected_features = mrmr_classif(X=None, y=Y_train, K=K)\n",
    "\n",
    "    print(f\"selected_features {selected_features}\")\n",
    "    # selected_features =['original_glcm_JointEntropy_adc', 'wavelet-HLH_firstorder_RobustMeanAbsoluteDeviation_adc', 'wavelet-LLL_firstorder_Kurtosis_adc', 'original_shape_Sphericity_adc', 'wavelet-LHL_firstorder_RootMeanSquared_hbv', 'original_glcm_SumEntropy_adc', 'log-sigma-3-0-mm-3D_glszm_SmallAreaEmphasis_adc']\n",
    "    \n",
    "    X_train=main_df_train[selected_features]\n",
    "    X_test=main_df_val[selected_features]\n",
    "\n",
    "\n",
    "\n",
    "    # # print(f\"yyyyyyyyy {Y_train.to_numpy().astype(int)}\")\n",
    "    ngb_cat = NGBClassifier(Dist=k_categorical(num_classes), verbose=True\n",
    "                            ,Base=Base\n",
    "                            ,n_estimators=n_estimators\n",
    "                            ,learning_rate=learning_rate\n",
    "                            ,minibatch_frac=minibatch_frac) \n",
    "    def classsssa():\n",
    "        try:\n",
    "            _ = ngb_cat.fit(X_train, Y_train.to_numpy().astype(int))\n",
    "        except:\n",
    "            try:\n",
    "                _ = ngb_cat.fit(X_train, Y_train.to_numpy().astype(int))\n",
    "            except:\n",
    "                try:\n",
    "                    _ = ngb_cat.fit(X_train, Y_train.to_numpy().astype(int))    \n",
    "                except:\n",
    "                    _ = ngb_cat.fit(X_train, Y_train.to_numpy().astype(int)) \n",
    "    def classsssb():                  \n",
    "\n",
    "        try:\n",
    "            classsssa()\n",
    "        except:\n",
    "            classsssa() \n",
    "    def classsss():                  \n",
    "\n",
    "        try:\n",
    "            classsssb()\n",
    "        except:\n",
    "            classsssb() \n",
    "            \n",
    "    try:\n",
    "        classsss()\n",
    "    except:\n",
    "        classsss()    \n",
    "    #     print(f\"error\")\n",
    "    #     return 0.0\n",
    "    \n",
    "    file_path = Path('/workspaces/pilot_lymphoma/data/ngbtest.p')\n",
    "\n",
    "    with file_path.open(\"wb\") as f:\n",
    "        pickle.dump(ngb_cat, f)\n",
    "\n",
    "    # with file_path.open(\"rb\") as f:\n",
    "    #     ngb_cat = pickle.load(f)\n",
    "\n",
    "    if(to_display):\n",
    "        #display feature importance\n",
    "        display_feature_importance(ngb_cat,X_train)\n",
    "\n",
    "\n",
    "    inferred=ngb_cat.predict(X_test)\n",
    "    # print(f\"iii {inferred}\")\n",
    "    # print(f\"iii2 {Y_test.to_numpy()}\")\n",
    "\n",
    "    acc=accuracy_score(Y_test.to_numpy(), inferred)\n",
    "    inferred_probs = ngb_cat.predict_proba(X_test)\n",
    "\n",
    "    # if(to_display):\n",
    "    # for curr_class in range(num_classes):\n",
    "    #     display_probs(curr_class, inferred_probs, Y_test)\n",
    "\n",
    "    # print(f\"probs {inferred_probs}\")\n",
    "    print(f\"\"\"AAA Accuracy: {acc}\"\"\")\n",
    "    # if(num_classes==2):\n",
    "    #     a=(inferred_probs[:,1]>0.7).astype(bool)\n",
    "    #     b=Y_test.to_numpy()\n",
    "    #     high_confidence=np.sum(np.logical_and(a,b).flatten())/np.sum(b.flatten())\n",
    "    #     print(f\"high_confidence {high_confidence}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    return acc,inferred_probs, Y_test\n",
    "#K is number of features we want to select\n",
    "# K=20\n",
    "\n",
    "# def classify_full(trial):\n",
    "def classify_full():\n",
    "    \n",
    "    # K=20\n",
    "    K=3\n",
    "    # X, y = make_classification(n_samples = 1000, n_features = 50, n_informative = 10, n_redundant = 40)\n",
    "    res_path=\"\"\n",
    "    radiomics_full_data_path=\"/workspaces/pilot_lymphoma/data/extracted_features_pet_trimmedB.csv\"\n",
    "    radiomics_full_data=pd.read_csv(radiomics_full_data_path)\n",
    "    radiomics_full_data = radiomics_full_data.loc[:, ~radiomics_full_data.columns.str.contains('Unnamed', case=False)]\n",
    "    radiomics_full_data = radiomics_full_data[radiomics_full_data['lesion_num'] == 1000]\n",
    "    radiomics_full_data[\"pat_id\"]=radiomics_full_data[\"pat_id\"].astype(int)\n",
    "    radiomics_full_data.columns\n",
    "\n",
    "    full_data_table_path=\"/workspaces/pilot_lymphoma/data/full_table_data_for_delta.csv\"\n",
    "    full_data_table= pd.read_csv(full_data_table_path)\n",
    "    full_data_table[\"pat_id\"]=full_data_table[\"Unnamed: 0\"].astype(int)\n",
    "    full_data_table[\"outcome\"]=full_data_table[\"Unnamed: 12\"]\n",
    "    # np.unique(full_data_table[\"outcome\"].to_numpy)\n",
    "    # Get first 20 percent of rows\n",
    "    rows = list(full_data_table.iterrows())\n",
    "    delta_r=list(map( lambda el: get_delta_radiomics(el, radiomics_full_data),rows))\n",
    "\n",
    "    delta_r= [el for el in delta_r if el != \" \"]\n",
    "    delta_r=pd.DataFrame(delta_r)\n",
    "\n",
    "    # print(f\"main_df_train {main_df_train['outcome']}\")\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    y_cols=[\"outcome\"]#,\"vol_in_mm3\"\n",
    "    # clinical_cols=[\"dre\",\"psa\",\"age\"]\n",
    "    # clinical_cols=[\"psa\",\"age\",\"dre\"]\n",
    "    # chosen_y_col=\"is_cancer\"\n",
    "    # chosen_y_col=\"isup\"\n",
    "    # chosen_y_col=\"isup_simple\"\n",
    "    chosen_y_col=\"outcome\"\n",
    "    # num_classes=2\n",
    "    num_classes=2\n",
    "\n",
    "    n_estimators=20#trial.suggest_int(\"n_estimators\", 100,2000)   \n",
    "    learning_rate=0.006639867572400997#trial.suggest_float(\"learning_rate\", 0.00001,0.1)   \n",
    "    minibatch_frac = 0.7561751607203051#trial.suggest_float(\"minibatch_frac\", 0.7,1.0) \n",
    "\n",
    "   \n",
    "    # clasify( main_df_val,main_df_train,y_cols,clinical_cols,chosen_y_col,num_classes,K)\n",
    "    # clasify( main_df_val,main_df_train,y_cols,clinical_cols,chosen_y_col,num_classes,K)\n",
    "    Base=get_tree_hyper_params([])    \n",
    "    \n",
    "    # Perform 5-fold cross validation\n",
    "    kf = KFold(n_splits=5, shuffle=True)\n",
    "\n",
    "    acc=[]\n",
    "    inferred_probs=[]\n",
    "    Y_test=[]\n",
    "    for train_index, val_index in kf.split(delta_r):\n",
    "        main_df_train = delta_r.iloc[train_index]\n",
    "        main_df_val = delta_r.iloc[val_index]\n",
    "\n",
    "\n",
    "        # main_df_val = delta_r.head(int(len(delta_r) * 0.2))\n",
    "        # main_df_train = delta_r.tail(int(len(delta_r) * 0.8))\n",
    "        \n",
    "        reACC,inferred_probS_CURR,Y_test_curr=clasify( main_df_val,main_df_train,y_cols,chosen_y_col,num_classes,K,False,Base,n_estimators,learning_rate,minibatch_frac)\n",
    "        \n",
    "        \n",
    "        inferred_probs.append(inferred_probS_CURR)\n",
    "        Y_test.append(Y_test_curr)\n",
    "        \n",
    "        acc.append(acc)\n",
    "        \n",
    "        # print(res[0])\n",
    "        \n",
    "    print(inferred_probs)\n",
    "    inferred_probs = np.concatenate(inferred_probs,axis=0)\n",
    "    Y_test = np.concatenate(Y_test,axis=0)\n",
    "    print(f\"mmmmmmmmean acc {np.mean(acc)}\")\n",
    "\n",
    "    print(f\"yyyyyyyyy inferred_probs {inferred_probs.shape} Y_test {Y_test.shape}\")\n",
    "    for curr_class in range(num_classes):\n",
    "        display_probs(curr_class, inferred_probs, Y_test)\n",
    "    \n",
    "    return np.mean(acc)\n",
    "\n",
    "\n",
    "    # in case of clasyfing isup we need to take a maximum of the isup values for each lesion\n",
    "\n",
    "classify_full()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
