{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00,  4.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selected_features ['wavelet-HHH_glcm_Imc1_pet', 'wavelet-HHL_glszm_GrayLevelNonUniformityNormalized_ct', 'log-sigma-5-0-mm-3D_firstorder_Kurtosis_pet']\n",
      "[iter 0] loss=0.4941 val_loss=0.0000 scale=4.0000 norm=7.1901\n",
      "AAA Accuracy: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00,  4.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selected_features ['wavelet-HHH_glcm_Imc2_pet', 'wavelet-LLL_glcm_Idn_ct', 'wavelet-LHH_glszm_LargeAreaLowGrayLevelEmphasis_ct']\n",
      "[iter 0] loss=0.3492 val_loss=0.0000 scale=512.0000 norm=814.5455\n",
      "[iter 0] loss=0.6012 val_loss=0.0000 scale=2.0000 norm=4.5455\n",
      "[iter 0] loss=0.6012 val_loss=0.0000 scale=512.0000 norm=1163.6364\n",
      "[iter 0] loss=0.6012 val_loss=0.0000 scale=2.0000 norm=4.5455\n",
      "[iter 0] loss=0.3492 val_loss=0.0000 scale=512.0000 norm=814.5455\n",
      "[iter 0] loss=0.4752 val_loss=0.0000 scale=512.0000 norm=989.0909\n",
      "[iter 0] loss=0.4752 val_loss=0.0000 scale=512.0000 norm=989.0909\n",
      "[iter 0] loss=0.6012 val_loss=0.0000 scale=8.0000 norm=18.1818\n",
      "[iter 0] loss=0.6012 val_loss=0.0000 scale=8.0000 norm=18.1818\n",
      "[iter 0] loss=0.4752 val_loss=0.0000 scale=512.0000 norm=989.0909\n",
      "AAA Accuracy: 0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00,  4.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selected_features ['wavelet-LHH_glszm_GrayLevelNonUniformityNormalized_pet', 'wavelet-LHH_glcm_DifferenceVariance_pet', 'log-sigma-4-0-mm-3D_glcm_DifferenceVariance_ct']\n",
      "[iter 0] loss=0.4941 val_loss=0.0000 scale=4.0000 norm=7.1901\n",
      "AAA Accuracy: 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00,  4.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selected_features ['wavelet-HHH_glcm_Imc2_pet', 'wavelet-HHH_glszm_SmallAreaLowGrayLevelEmphasis_pet', 'wavelet-HLH_glcm_Imc1_pet']\n",
      "[iter 0] loss=0.5945 val_loss=0.0000 scale=2.0000 norm=3.8182\n",
      "[iter 0] loss=0.7205 val_loss=0.0000 scale=512.0000 norm=1117.0909\n",
      "[iter 0] loss=0.6575 val_loss=0.0000 scale=4.0000 norm=8.1818\n",
      "AAA Accuracy: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00,  4.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selected_features ['wavelet-HLH_glcm_Imc1_pet', 'wavelet-LHL_glszm_SmallAreaEmphasis_pet', 'wavelet-HHH_firstorder_Entropy_pet']\n",
      "[iter 0] loss=0.4708 val_loss=0.0000 scale=8.0000 norm=14.2222\n",
      "[iter 0] loss=0.4708 val_loss=0.0000 scale=8.0000 norm=14.2222\n",
      "[iter 0] loss=0.4708 val_loss=0.0000 scale=2.0000 norm=3.5556\n",
      "AAA Accuracy: 0.6666666666666666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import SimpleITK as sitk\n",
    "import six\n",
    "from radiomics import featureextractor, getTestCase\n",
    "import multiprocessing\n",
    "import optuna\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from ngboost import NGBRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from ngboost.distns import Exponential, Normal\n",
    "from ngboost import NGBClassifier\n",
    "from ngboost.distns import k_categorical, Bernoulli\n",
    "from mrmr import mrmr_classif\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import shap\n",
    "import sklearn\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "def subtract_dicts(dict1, dict2,dict_sums):\n",
    "    # Create a new dictionary with the absolute difference of each entry\n",
    "    result = {key: abs(dict1[key] - dict2[key])/dict_sums[key] for key in dict1}\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "def get_delta_radiomics(full_data_table_row, radiomics_full_data):\n",
    "\n",
    "    \"\"\"a function 'get_delta_radiomics' that would have two arguments 'full_data_table_row' and 'radiomics_full_data'.  'full_data_table_row' is a row from main table and contains columns like '[pat_id,outcome]'   'radiomics_full_data' contains multiple column including'[pat_id,study_0_or_1]'  Function should perform all steps:\n",
    "    1) find 2 rows from 'radiomics_full_data' where  value of column 'pat_id' would be the same as value of column 'pat_id' in 'full_data_table_row' \n",
    "    2) From those 2 rows you found drop columns with names: ```['pat_id', 'lesion_num', 'study_0_or_1', 'Deauville', 'lab_path', 'mod_name']```\n",
    "    3) Save the sum of both rows so each column should have sum of 2 rows\n",
    "    4) calculate the absolute value of the diffrence between two rows and divide it by the saved sum save information as dictionary called 'res'\n",
    "    5) add outcome variable to 'res' that you will find in column 'outcome' in 'full_data_table_row' encode the  'outcome' as integer as in the dictionary {'CR':0, 'PD':1, 'PR':2, 'SD':2, }\n",
    "    6) return calculated dictionary res\"\"\"\n",
    "    full_data_table_row=full_data_table_row[1]\n",
    "    # print(f\"pppp {full_data_table_row['pat_id']}\")\n",
    "    # Step 1\n",
    "    rows = radiomics_full_data[radiomics_full_data['pat_id'] == full_data_table_row['pat_id']]\n",
    "    if(len(rows))<2:\n",
    "        # print(f\"ggggggggggggg {len(rows)}  {full_data_table_row['pat_id']}\")\n",
    "        return \" \"\n",
    "    # Step 2\n",
    "    rows = rows.drop(columns=['pat_id', 'lesion_num', 'study_0_or_1', 'Deauville', 'lab_path', 'mod_name'])\n",
    "    # Step 3\n",
    "    row_sum = rows.sum().to_dict()\n",
    "   \n",
    "    # print(len(rows))\n",
    "    # Step 4\n",
    "    res = subtract_dicts(rows.iloc[0].to_dict(),rows.iloc[1].to_dict(),row_sum )\n",
    "    # print(res)\n",
    "    # Step 5\n",
    "    outcome_dict = {'CR':0, 'PD':1, 'PR':0, 'SD':0}\n",
    "    res['outcome'] = outcome_dict[full_data_table_row['outcome']]\n",
    "\n",
    "    # Step 6\n",
    "    return res\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# mrmr_selection,shap,ngboost\n",
    "\n",
    "\n",
    "def display_probs(curr_class, inferred_probs, Y_test,to_be_sorted=True):\n",
    "\n",
    "    probd_curr=inferred_probs[:,curr_class]\n",
    "    class_curr=(Y_test==curr_class)\n",
    "    if(to_be_sorted):\n",
    "        # Concatenate probd_curr and class_curr\n",
    "        combined = np.column_stack((probd_curr, class_curr))\n",
    "\n",
    "        # Sort by probd_curr\n",
    "        combined_sorted = combined[combined[:, 0].argsort()[::-1]]\n",
    "\n",
    "        # Divide back into probd_curr and class_curr\n",
    "        probd_curr = combined_sorted[:, 0]\n",
    "        class_curr = combined_sorted[:, 1]\n",
    "\n",
    "    # Set the colors for the columns\n",
    "    colors = ['red' if c == 0 else 'green' for c in class_curr]\n",
    "    # Plot the column plot\n",
    "    plt.bar(range(len(probd_curr)), probd_curr, color=colors)\n",
    "\n",
    "    # Add a vertical line at value 0.5\n",
    "    plt.axhline(y=0.5, color='blue', linestyle='--')\n",
    "\n",
    "\n",
    "\n",
    "    # Add labels and title\n",
    "    plt.xlabel('Index')\n",
    "    plt.ylabel('Probability')\n",
    "    plt.title(f'class {curr_class}')\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "def display_feature_importance(ngb,X_train):\n",
    "\n",
    "    shap.initjs()\n",
    "\n",
    "    ## SHAP plot for loc trees\n",
    "    explainer = shap.TreeExplainer(ngb, model_output=0) # use model_output = 1 for scale trees\n",
    "    shap_values = explainer.shap_values(X_train)\n",
    "    shap.summary_plot(shap_values, X_train, feature_names=X_train.columns.to_numpy())\n",
    "\n",
    "\n",
    "\n",
    "def get_tree_hyper_params(trial):\n",
    "    criterion= \"friedman_mse\"#trial.suggest_categorical(\"criterion\", [\"squared_error\", \"friedman_mse\", \"absolute_error\", \"poisson\"])\n",
    "    splitter=\"random\"#trial.suggest_categorical(\"splitter\", [\"best\",\"random\"])\n",
    "    max_features=None#trial.suggest_categorical(\"max_features\", [\"sqrt\",\"log2\",None])\n",
    "    # max_leaf_nodesint=trial.suggest_categorical(\"max_leaf_nodesint\", [])\n",
    "    max_depth=2#trial.suggest_int(\"max_depth\", 1,10)\n",
    "    min_samples_leaf=1#trial.suggest_int(\"min_samples_leaf\", 1,3)\n",
    "    min_impurity_decrease= 0.2307277162959608#trial.suggest_float(\"min_impurity_decrease\", 0.0,0.3)\n",
    "\n",
    "    return sklearn.tree.DecisionTreeRegressor(criterion=criterion,splitter=splitter,max_depth=max_depth,max_features=max_features,min_samples_leaf=min_samples_leaf\n",
    "                                              ,min_impurity_decrease=min_impurity_decrease)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def clasify( main_df_val,main_df_train,y_cols,chosen_y_col,num_classes,K,to_display,Base,n_estimators,learning_rate,minibatch_frac):\n",
    "\n",
    "    main_df_val[chosen_y_col]=main_df_val[chosen_y_col].to_numpy().astype(int)\n",
    "    main_df_train[chosen_y_col]=main_df_train[chosen_y_col].to_numpy().astype(int)\n",
    "    \n",
    "    # main_df_val = main_df_val[main_df_val[chosen_y_col] > -1]\n",
    "    # main_df_train = main_df_train[main_df_train[chosen_y_col] > -1]\n",
    "\n",
    "    # print(f\"mmmm val {len(main_df_val)} train {len(main_df_train)}\")\n",
    "\n",
    "    X_train = main_df_train.drop(columns=y_cols )\n",
    "    X_test = main_df_val.drop(columns=y_cols)\n",
    "\n",
    "    X_train = X_train.iloc[:, 1:]\n",
    "    X_test = X_train.iloc[:, 1:]\n",
    "\n",
    "    Y_train = main_df_train[chosen_y_col]\n",
    "    Y_test = main_df_val[chosen_y_col]\n",
    "\n",
    "\n",
    "    # select top K features using mRMR\n",
    "    selected_features = mrmr_classif(X=X_train, y=Y_train, K=3,n_jobs=1)\n",
    "    # selected_features = mrmr_classif(X=None, y=Y_train, K=K)\n",
    "\n",
    "    print(f\"selected_features {selected_features}\")\n",
    "    # selected_features =['original_glcm_JointEntropy_adc', 'wavelet-HLH_firstorder_RobustMeanAbsoluteDeviation_adc', 'wavelet-LLL_firstorder_Kurtosis_adc', 'original_shape_Sphericity_adc', 'wavelet-LHL_firstorder_RootMeanSquared_hbv', 'original_glcm_SumEntropy_adc', 'log-sigma-3-0-mm-3D_glszm_SmallAreaEmphasis_adc']\n",
    "    \n",
    "    X_train=main_df_train[selected_features]\n",
    "    X_test=main_df_val[selected_features]\n",
    "\n",
    "\n",
    "\n",
    "    # # print(f\"yyyyyyyyy {Y_train.to_numpy().astype(int)}\")\n",
    "    ngb_cat = NGBClassifier(Dist=k_categorical(num_classes), verbose=True\n",
    "                            ,Base=Base\n",
    "                            ,n_estimators=n_estimators\n",
    "                            ,learning_rate=learning_rate\n",
    "                            ,minibatch_frac=minibatch_frac) \n",
    "    def classsssa():\n",
    "        try:\n",
    "            _ = ngb_cat.fit(X_train, Y_train.to_numpy().astype(int))\n",
    "        except:\n",
    "            try:\n",
    "                _ = ngb_cat.fit(X_train, Y_train.to_numpy().astype(int))\n",
    "            except:\n",
    "                try:\n",
    "                    _ = ngb_cat.fit(X_train, Y_train.to_numpy().astype(int))    \n",
    "                except:\n",
    "                    _ = ngb_cat.fit(X_train, Y_train.to_numpy().astype(int)) \n",
    "    def classsssb():                  \n",
    "\n",
    "        try:\n",
    "            classsssa()\n",
    "        except:\n",
    "            classsssa() \n",
    "    def classsss():                  \n",
    "\n",
    "        try:\n",
    "            classsssb()\n",
    "        except:\n",
    "            classsssb() \n",
    "            \n",
    "    try:\n",
    "        classsss()\n",
    "    except:\n",
    "        classsss()    \n",
    "    #     print(f\"error\")\n",
    "    #     return 0.0\n",
    "    \n",
    "    file_path = Path('/workspaces/pilot_lymphoma/data/ngbtest.p')\n",
    "\n",
    "    with file_path.open(\"wb\") as f:\n",
    "        pickle.dump(ngb_cat, f)\n",
    "\n",
    "    # with file_path.open(\"rb\") as f:\n",
    "    #     ngb_cat = pickle.load(f)\n",
    "\n",
    "    # if(to_display):\n",
    "    #     #display feature importance\n",
    "    #     display_feature_importance(ngb_cat,X_train)\n",
    "\n",
    "\n",
    "    inferred=ngb_cat.predict(X_test)\n",
    "    # print(f\"iii {inferred}\")\n",
    "    # print(f\"iii2 {Y_test.to_numpy()}\")\n",
    "\n",
    "    acc=accuracy_score(Y_test.to_numpy(), inferred)\n",
    "    inferred_probs = ngb_cat.predict_proba(X_test)\n",
    "\n",
    "    # if(to_display):\n",
    "    # for curr_class in range(num_classes):\n",
    "    #     display_probs(curr_class, inferred_probs, Y_test)\n",
    "\n",
    "    # print(f\"probs {inferred_probs}\")\n",
    "    print(f\"\"\"AAA Accuracy: {acc}\"\"\")\n",
    "    # if(num_classes==2):\n",
    "    #     a=(inferred_probs[:,1]>0.7).astype(bool)\n",
    "    #     b=Y_test.to_numpy()\n",
    "    #     high_confidence=np.sum(np.logical_and(a,b).flatten())/np.sum(b.flatten())\n",
    "    #     print(f\"high_confidence {high_confidence}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    return acc,inferred_probs, Y_test\n",
    "#K is number of features we want to select\n",
    "# K=20\n",
    "\n",
    "# def classify_full(trial):\n",
    "def classify_full():\n",
    "    \n",
    "    # K=20\n",
    "    K=3\n",
    "    # X, y = make_classification(n_samples = 1000, n_features = 50, n_informative = 10, n_redundant = 40)\n",
    "    res_path=\"\"\n",
    "    radiomics_full_data_path=\"/workspaces/pilot_lymphoma/data/extracted_features_pet_trimmedB.csv\"\n",
    "    radiomics_full_data=pd.read_csv(radiomics_full_data_path)\n",
    "    radiomics_full_data = radiomics_full_data.loc[:, ~radiomics_full_data.columns.str.contains('Unnamed', case=False)]\n",
    "    radiomics_full_data = radiomics_full_data[radiomics_full_data['lesion_num'] == 1000]\n",
    "    radiomics_full_data[\"pat_id\"]=radiomics_full_data[\"pat_id\"].astype(int)\n",
    "    radiomics_full_data.columns\n",
    "\n",
    "    full_data_table_path=\"/workspaces/pilot_lymphoma/data/full_table_data_for_delta.csv\"\n",
    "    full_data_table= pd.read_csv(full_data_table_path)\n",
    "    full_data_table[\"pat_id\"]=full_data_table[\"Unnamed: 0\"].astype(int)\n",
    "    full_data_table[\"outcome\"]=full_data_table[\"Unnamed: 12\"]\n",
    "    # np.unique(full_data_table[\"outcome\"].to_numpy)\n",
    "    # Get first 20 percent of rows\n",
    "    rows = list(full_data_table.iterrows())\n",
    "    delta_r=list(map( lambda el: get_delta_radiomics(el, radiomics_full_data),rows))\n",
    "\n",
    "    delta_r= [el for el in delta_r if el != \" \"]\n",
    "    delta_r=pd.DataFrame(delta_r)\n",
    "\n",
    "    # print(f\"main_df_train {main_df_train['outcome']}\")\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    y_cols=[\"outcome\"]#,\"vol_in_mm3\"\n",
    "    # clinical_cols=[\"dre\",\"psa\",\"age\"]\n",
    "    # clinical_cols=[\"psa\",\"age\",\"dre\"]\n",
    "    # chosen_y_col=\"is_cancer\"\n",
    "    # chosen_y_col=\"isup\"\n",
    "    # chosen_y_col=\"isup_simple\"\n",
    "    chosen_y_col=\"outcome\"\n",
    "    # num_classes=2\n",
    "    num_classes=2\n",
    "\n",
    "    n_estimators=20#trial.suggest_int(\"n_estimators\", 100,2000)   \n",
    "    learning_rate=0.006639867572400997#trial.suggest_float(\"learning_rate\", 0.00001,0.1)   \n",
    "    minibatch_frac = 0.7561751607203051#trial.suggest_float(\"minibatch_frac\", 0.7,1.0) \n",
    "\n",
    "   \n",
    "    # clasify( main_df_val,main_df_train,y_cols,clinical_cols,chosen_y_col,num_classes,K)\n",
    "    # clasify( main_df_val,main_df_train,y_cols,clinical_cols,chosen_y_col,num_classes,K)\n",
    "    Base=get_tree_hyper_params([])    \n",
    "    \n",
    "    # Perform 5-fold cross validation\n",
    "    kf = KFold(n_splits=5, shuffle=True)\n",
    "\n",
    "    acc=[]\n",
    "    inferred_probs=[]\n",
    "    Y_test=[]\n",
    "    for train_index, val_index in kf.split(delta_r):\n",
    "        main_df_train = delta_r.iloc[train_index]\n",
    "        main_df_val = delta_r.iloc[val_index]\n",
    "\n",
    "\n",
    "        # main_df_val = delta_r.head(int(len(delta_r) * 0.2))\n",
    "        # main_df_train = delta_r.tail(int(len(delta_r) * 0.8))\n",
    "        \n",
    "        reACC,inferred_probS_CURR,Y_test_curr=clasify( main_df_val,main_df_train,y_cols,chosen_y_col,num_classes,K,False,Base,n_estimators,learning_rate,minibatch_frac)\n",
    "        \n",
    "        \n",
    "        inferred_probs.append(inferred_probS_CURR)\n",
    "        Y_test.append(Y_test_curr)\n",
    "        \n",
    "        acc.append(reACC)\n",
    "        \n",
    "        # print(res[0])\n",
    "        \n",
    "    # print(inferred_probs)\n",
    "    # inferred_probs = np.concatenate(inferred_probs,axis=0)\n",
    "    # Y_test = np.concatenate(Y_test,axis=0)\n",
    "    # print(f\"mmmmmmmmean acc {np.mean(acc)}\")\n",
    "\n",
    "    # print(f\"yyyyyyyyy inferred_probs {inferred_probs.shape} Y_test {Y_test.shape}\")\n",
    "    # for curr_class in range(num_classes):\n",
    "    #     display_probs(curr_class, inferred_probs, Y_test)\n",
    "    \n",
    "    # return np.mean(acc)\n",
    "    return acc\n",
    "\n",
    "\n",
    "    # in case of clasyfing isup we need to take a maximum of the isup values for each lesion\n",
    "\n",
    "mean_acc=classify_full()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7833333333333333"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(mean_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
