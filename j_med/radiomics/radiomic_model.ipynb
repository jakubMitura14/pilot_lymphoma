{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:01<00:00,  3.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selected_features ['wavelet-HHH_glcm_JointAverage_ct', 'wavelet-HLH_glcm_Autocorrelation_ct', 'wavelet-HLL_glcm_ClusterProminence_pet', 'wavelet-HHL_glcm_DifferenceVariance_ct', 'wavelet-HHH_firstorder_Median_pet', 'wavelet-LLH_glszm_ZoneVariance_ct', 'wavelet-HHH_glcm_SumAverage_ct']\n",
      "yyyyyyyyy Y_train 36  [0 1] X_train 36\n",
      "[iter 0] loss=0.6787 val_loss=0.0000 scale=8.0000 norm=15.8667\n",
      "Accuracy: 0.5555555555555556\n",
      "high_confidence 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:01<00:00,  3.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selected_features ['wavelet-HHH_glszm_SmallAreaHighGrayLevelEmphasis_ct', 'wavelet-HHL_glszm_GrayLevelNonUniformityNormalized_pet', 'wavelet-LHL_glcm_ClusterShade_ct', 'wavelet-HHH_gldm_LowGrayLevelEmphasis_ct', 'wavelet-HHH_glszm_HighGrayLevelZoneEmphasis_ct', 'wavelet-HLL_glszm_SizeZoneNonUniformity_ct', 'wavelet-LLL_firstorder_Kurtosis_pet']\n",
      "yyyyyyyyy Y_train 36  [0 1] X_train 36\n",
      "[iter 0] loss=0.6931 val_loss=0.0000 scale=4.0000 norm=8.0000\n",
      "Accuracy: 0.3333333333333333\n",
      "high_confidence 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:01<00:00,  3.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selected_features ['wavelet-LLH_firstorder_Median_ct', 'log-sigma-3-0-mm-3D_glszm_ZoneVariance_pet', 'wavelet-HLH_gldm_DependenceNonUniformityNormalized_pet', 'wavelet-HHH_glszm_LowGrayLevelZoneEmphasis_ct', 'wavelet-HHH_gldm_HighGrayLevelEmphasis_ct', 'log-sigma-3-0-mm-3D_firstorder_Kurtosis_ct', 'wavelet-LHH_glszm_SizeZoneNonUniformityNormalized_pet']\n",
      "yyyyyyyyy Y_train 36  [0 1] X_train 36\n",
      "[iter 0] loss=0.6636 val_loss=0.0000 scale=2.0000 norm=3.9365\n",
      "Accuracy: 0.4444444444444444\n",
      "high_confidence 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:01<00:00,  3.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selected_features ['wavelet-HHH_gldm_LowGrayLevelEmphasis_ct', 'wavelet-HLL_gldm_GrayLevelVariance_pet', 'log-sigma-3-0-mm-3D_gldm_LargeDependenceHighGrayLevelEmphasis_ct', 'wavelet-HHH_gldm_HighGrayLevelEmphasis_ct', 'wavelet-LLL_firstorder_Kurtosis_pet', 'wavelet-HHH_glszm_HighGrayLevelZoneEmphasis_ct', 'wavelet-HHH_firstorder_RootMeanSquared_pet']\n",
      "yyyyyyyyy Y_train 36  [0 1] X_train 36\n",
      "[iter 0] loss=0.6885 val_loss=0.0000 scale=4.0000 norm=8.0762\n",
      "Accuracy: 0.4444444444444444\n",
      "high_confidence 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:01<00:00,  3.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selected_features ['wavelet-LLH_firstorder_Median_ct', 'wavelet-HHL_glcm_ClusterProminence_pet', 'wavelet-HHH_glszm_SizeZoneNonUniformityNormalized_pet', 'wavelet-HHH_glcm_JointAverage_ct', 'log-sigma-3-0-mm-3D_glszm_LargeAreaHighGrayLevelEmphasis_pet', 'wavelet-LLH_firstorder_Kurtosis_ct', 'wavelet-LLL_glcm_Imc2_pet']\n",
      "yyyyyyyyy Y_train 36  [0 1] X_train 36\n",
      "[iter 0] loss=0.6787 val_loss=0.0000 scale=2.0000 norm=3.9667\n",
      "Accuracy: 0.5555555555555556\n",
      "high_confidence 0.0\n",
      "whole_res 0.4666666666666666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import SimpleITK as sitk\n",
    "import six\n",
    "from radiomics import featureextractor, getTestCase\n",
    "import multiprocessing\n",
    "import optuna\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from ngboost import NGBRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from ngboost.distns import Exponential, Normal\n",
    "from ngboost import NGBClassifier\n",
    "from ngboost.distns import k_categorical, Bernoulli\n",
    "from mrmr import mrmr_classif\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import shap\n",
    "import sklearn\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import KFold\n",
    "# mrmr_selection,shap,ngboost\n",
    "\n",
    "\n",
    "def display_probs(curr_class, inferred_probs, Y_test,to_be_sorted=True):\n",
    "\n",
    "    probd_curr=inferred_probs[:,curr_class]\n",
    "    class_curr=(Y_test==curr_class).to_numpy().astype(int)\n",
    "    if(to_be_sorted):\n",
    "        # Concatenate probd_curr and class_curr\n",
    "        combined = np.column_stack((probd_curr, class_curr))\n",
    "\n",
    "        # Sort by probd_curr\n",
    "        combined_sorted = combined[combined[:, 0].argsort()[::-1]]\n",
    "\n",
    "        # Divide back into probd_curr and class_curr\n",
    "        probd_curr = combined_sorted[:, 0]\n",
    "        class_curr = combined_sorted[:, 1]\n",
    "\n",
    "    # Set the colors for the columns\n",
    "    colors = ['red' if c == 0 else 'green' for c in class_curr]\n",
    "    # Plot the column plot\n",
    "    plt.bar(range(len(probd_curr)), probd_curr, color=colors)\n",
    "\n",
    "    # Add a vertical line at value 0.5\n",
    "    plt.axhline(y=0.5, color='blue', linestyle='--')\n",
    "\n",
    "\n",
    "\n",
    "    # Add labels and title\n",
    "    plt.xlabel('Index')\n",
    "    plt.ylabel('Probability')\n",
    "    plt.title(f'class {curr_class}')\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "def display_feature_importance(ngb,X_train):\n",
    "\n",
    "    shap.initjs()\n",
    "\n",
    "    ## SHAP plot for loc trees\n",
    "    explainer = shap.TreeExplainer(ngb, model_output=0) # use model_output = 1 for scale trees\n",
    "    shap_values = explainer.shap_values(X_train)\n",
    "    shap.summary_plot(shap_values, X_train, feature_names=X_train.columns.to_numpy())\n",
    "\n",
    "\n",
    "\n",
    "def get_tree_hyper_params(trial):\n",
    "    criterion= \"friedman_mse\"#trial.suggest_categorical(\"criterion\", [\"squared_error\", \"friedman_mse\", \"absolute_error\", \"poisson\"])\n",
    "    splitter=\"random\"#trial.suggest_categorical(\"splitter\", [\"best\",\"random\"])\n",
    "    max_features=None#trial.suggest_categorical(\"max_features\", [\"sqrt\",\"log2\",None])\n",
    "    # max_leaf_nodesint=trial.suggest_categorical(\"max_leaf_nodesint\", [])\n",
    "    max_depth=3#trial.suggest_int(\"max_depth\", 1,10)\n",
    "    min_samples_leaf=1#trial.suggest_int(\"min_samples_leaf\", 1,3)\n",
    "    min_impurity_decrease= 0.2307277162959608#trial.suggest_float(\"min_impurity_decrease\", 0.0,0.3)\n",
    "\n",
    "    return sklearn.tree.DecisionTreeRegressor(criterion=criterion,splitter=splitter,max_depth=max_depth,max_features=max_features,min_samples_leaf=min_samples_leaf\n",
    "                                              ,min_impurity_decrease=min_impurity_decrease)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def clasify( main_df_val,main_df_train,y_cols,chosen_y_col,num_classes,K,to_display,Base,n_estimators,learning_rate,minibatch_frac):\n",
    "\n",
    "    main_df_val[chosen_y_col]=main_df_val[chosen_y_col].to_numpy().astype(int)\n",
    "    main_df_train[chosen_y_col]=main_df_train[chosen_y_col].to_numpy().astype(int)\n",
    "    \n",
    "    # main_df_val = main_df_val[main_df_val[chosen_y_col] > -1]\n",
    "    # main_df_train = main_df_train[main_df_train[chosen_y_col] > -1]\n",
    "\n",
    "    # print(f\"mmmm val {len(main_df_val)} train {len(main_df_train)}\")\n",
    "\n",
    "    X_train = main_df_train.drop(columns=y_cols )\n",
    "    X_test = main_df_val.drop(columns=y_cols)\n",
    "\n",
    "    X_train = X_train.iloc[:, 1:]\n",
    "    X_test = X_train.iloc[:, 1:]\n",
    "\n",
    "    Y_train = main_df_train[chosen_y_col]\n",
    "    Y_test = main_df_val[chosen_y_col]\n",
    "\n",
    "    # print(f\"X_train {X_train} Y_train {Y_train}\")\n",
    "\n",
    "    # select top K features using mRMR\n",
    "    selected_features = mrmr_classif(X=X_train, y=Y_train, K=7,n_jobs=1)\n",
    "    # selected_features = mrmr_classif(X=None, y=Y_train, K=K)\n",
    "\n",
    "    print(f\"selected_features {selected_features}\")\n",
    "    # selected_features =['original_glcm_JointEntropy_adc', 'wavelet-HLH_firstorder_RobustMeanAbsoluteDeviation_adc', 'wavelet-LLL_firstorder_Kurtosis_adc', 'original_shape_Sphericity_adc', 'wavelet-LHL_firstorder_RootMeanSquared_hbv', 'original_glcm_SumEntropy_adc', 'log-sigma-3-0-mm-3D_glszm_SmallAreaEmphasis_adc']\n",
    "    \n",
    "    X_train=main_df_train[selected_features]\n",
    "    X_test=main_df_val[selected_features]\n",
    "\n",
    "\n",
    "\n",
    "    print(f\"yyyyyyyyy Y_train {len(Y_train)}  {np.unique(Y_train)} X_train {len(X_train)}\")\n",
    "    ngb_cat = NGBClassifier(Dist=k_categorical(num_classes), verbose=True\n",
    "                            ,Base=Base\n",
    "                            ,n_estimators=n_estimators\n",
    "                            ,learning_rate=learning_rate\n",
    "                            ,minibatch_frac=minibatch_frac) \n",
    "    # try:\n",
    "    _ = ngb_cat.fit(X_train, Y_train.to_numpy().astype(int))\n",
    "    # except:\n",
    "    #     print(f\"error\")\n",
    "    #     return 0.0\n",
    "    \n",
    "    file_path = Path('/workspaces/pilot_lymphoma/data/ngbtest.p')\n",
    "\n",
    "    with file_path.open(\"wb\") as f:\n",
    "        pickle.dump(ngb_cat, f)\n",
    "\n",
    "    # with file_path.open(\"rb\") as f:\n",
    "    #     ngb_cat = pickle.load(f)\n",
    "    # to_display=True\n",
    "    if(to_display):\n",
    "        #display feature importance\n",
    "        display_feature_importance(ngb_cat,X_train)\n",
    "\n",
    "\n",
    "    inferred=ngb_cat.predict(X_test)\n",
    "    # print(f\"iii {inferred}\")\n",
    "    # print(f\"iii2 {Y_test.to_numpy()}\")\n",
    "\n",
    "    acc=accuracy_score(Y_test.to_numpy(), inferred)\n",
    "    inferred_probs = ngb_cat.predict_proba(X_test)\n",
    "\n",
    "    if(to_display):\n",
    "        for curr_class in range(num_classes):\n",
    "            display_probs(curr_class, inferred_probs, Y_test)\n",
    "\n",
    "    # print(f\"probs {inferred_probs}\")\n",
    "    print(f\"\"\"Accuracy: {acc}\"\"\")\n",
    "    if(num_classes==2):\n",
    "        a=(inferred_probs[:,1]>0.7).astype(bool)\n",
    "        b=Y_test.to_numpy()\n",
    "        high_confidence=np.sum(np.logical_and(a,b).flatten())/np.sum(b.flatten())\n",
    "        print(f\"high_confidence {high_confidence}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    return acc\n",
    "#K is number of features we want to select\n",
    "# K=20\n",
    "\n",
    "# def classify_full(trial):\n",
    "def classify_full():\n",
    "    \n",
    "    # K=20\n",
    "    K=4\n",
    "    # X, y = make_classification(n_samples = 1000, n_features = 50, n_informative = 10, n_redundant = 40)\n",
    "    res_path=\"\"\n",
    "    main_df=pd.read_csv(\"/workspaces/pilot_lymphoma/data/extracted_features_pet_trimmedB.csv\")\n",
    "\n",
    "    # Get first 20 percent of rows\n",
    "    # main_df_val = main_df.head(int(len(main_df) * 0.2))\n",
    "    # main_df_train = main_df.tail(int(len(main_df) * 0.8))\n",
    "\n",
    "    y_cols=[\"pat_id\",\"lesion_num\",\"study_0_or_1\",\"Deauville\",\"lab_path\",\"mod_name\"]#,\"vol_in_mm3\"\n",
    "    main_df = main_df.loc[:, ~main_df.columns.str.contains('Unnamed')]\n",
    "    # clinical_cols=[\"dre\",\"psa\",\"age\"]\n",
    "    # clinical_cols=[\"psa\",\"age\",\"dre\"]\n",
    "    # chosen_y_col=\"is_cancer\"\n",
    "    # chosen_y_col=\"isup\"\n",
    "    # chosen_y_col=\"isup_simple\"\n",
    "    main_df[\"Deauville\"]=(main_df[\"Deauville\"].astype(int).to_numpy()>3)\n",
    "    main_df[\"Deauville\"]=main_df[\"Deauville\"].astype(int)\n",
    "    chosen_y_col=\"Deauville\"\n",
    "    # num_classes=2\n",
    "    num_classes=2\n",
    "\n",
    "    n_estimators=4#trial.suggest_int(\"n_estimators\", 100,2000)   \n",
    "    learning_rate=0.002639867572400997#trial.suggest_float(\"learning_rate\", 0.00001,0.1)   \n",
    "    minibatch_frac = 0.7561751607203051#trial.suggest_float(\"minibatch_frac\", 0.7,1.0) \n",
    "    kf = KFold(n_splits=5, shuffle=True)\n",
    "    res_list=[]\n",
    "    for train_index, val_index in kf.split(main_df):\n",
    "        main_df_train = main_df.iloc[train_index]\n",
    "        main_df_val = main_df.iloc[val_index]\n",
    "\n",
    "        # clasify( main_df_val,main_df_train,y_cols,clinical_cols,chosen_y_col,num_classes,K)\n",
    "        # clasify( main_df_val,main_df_train,y_cols,clinical_cols,chosen_y_col,num_classes,K)\n",
    "        Base=get_tree_hyper_params([])    \n",
    "        res=clasify( main_df_val,main_df_train,y_cols,chosen_y_col,num_classes,K,False,Base,n_estimators,learning_rate,minibatch_frac)\n",
    "        res_list.append(res)\n",
    "    return np.mean(res_list)\n",
    "\n",
    "\n",
    "    # in case of clasyfing isup we need to take a maximum of the isup values for each lesion\n",
    "\n",
    "whole_res=classify_full()\n",
    "print(f\"whole_res {np.mean(whole_res)}\")\n",
    "\n",
    "# database_name=\"nat\"\n",
    "# experiment_name=\"nat_199\"\n",
    "# # storage = optuna.storages.RDBStorage(\n",
    "# #     url=f\"mysql://root@34.90.134.17/{database_name}\",\n",
    "# #     # engine_kwargs={\"pool_size\": 20, \"connect_args\": {\"timeout\": 10}},\n",
    "# # )\n",
    "\n",
    "# study = optuna.create_study(\n",
    "#         study_name=experiment_name\n",
    "#         # ,sampler=optuna.samplers.CmaEsSampler()    \n",
    "#         ,sampler=optuna.samplers.NSGAIISampler()    \n",
    "#         # ,pruner=optuna.pruners.HyperbandPruner()\n",
    "#         # ,storage=f\"mysql://root:jm@34.90.134.17:3306/{experiment_name}\"\n",
    "#         # ,storage=f\"mysql://root@34.90.134.17/{database_name}\"\n",
    "#         # ,load_if_exists=True\n",
    "#         ,direction=\"maximize\"\n",
    "#         )\n",
    "\n",
    "# study.optimize(classify_full, n_trials=90000,gc_after_trial=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
